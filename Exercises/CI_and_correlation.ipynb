{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb27e67-eac4-45e4-b0e3-57d3381e34b8",
   "metadata": {},
   "source": [
    "# Error propagation and confidence intervals for correlated data points \n",
    "\n",
    "You have the data file \"data_cov.dat\" which contains 10 pairs of measurements $(a_i, b_i)$. You consider reporting the mean and standard deviation on $a$ and $b$. Is this information sufficient to calculate a 99% confidence interval on $\\bar{c} = \\bar{a} + \\bar{b}$? Explain why this is not sufficient and show it through a numerical experiment.   \n",
    "\n",
    "Bonus: Calculate a 99\\% CI on $\\bar{c}$. \n",
    "\n",
    "Tip: Instead of calculating the sample covariance manually, you can use `np.cov(a, b, ddof=1)`. \n",
    "\n",
    "Guidline for solving this exercise: \n",
    "\n",
    "- 1) Visualise the pairs of data points $(a_i, b_i)$\n",
    "- 2) Evaluate the covariance matrix of your sample and pearson correlation coefficient.\n",
    "- 3) Derive a p-value for your observed correlation coefficient. Is the correlation between a and b meaningful?\n",
    "- 4) Compare the variance on $c = a+b$ to the variance you would derive from the error propagation formula. Compare the results if you include of drop the covariance between a and b in the error propagation formula. \n",
    "\n",
    "## About correlation coefficients\n",
    "\n",
    "Given two length-$N$ samples of data $\\{x_i\\}$ and $\\{y_i\\}$, Pearson's correlation coefficient is defined as\n",
    "\n",
    "$$ r = \\frac{\\sum_{i=1}^N (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^N (x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^N (y_i-\\bar{y})^2}} $$\n",
    "\n",
    "where $-1\\leq r\\leq 1$, and $r=0$ for uncorrelated variables.\n",
    "\n",
    "If the pairs $(x_i,y_i)$ are drawn from uncorrelated univariate Gaussian distributions, then the distribution of $r$ follows a Student's $t$ distribution with $k=N-2$ degrees of freedom and $t = r\\sqrt{(N-2)/(1-r^2)}$ (so this is the normalised RV $r$ built based on t that follows a student ; this also means that the $stde(r) = \\sqrt{\\frac{1-r^2}{N-2}}$).\n",
    "\n",
    "Because of this, a measured $r$ can be interpreted in terms of the significance with which we can reject the variables being correlated. \n",
    "E.g., for $N=10$ you can estiamte probability that a value of $r$ arises just by chance noise fluctuations using a Student $t$ distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58b3b6-2716-4a29-8332-5149bde95c94",
   "metadata": {},
   "source": [
    "### About p-value \n",
    "\n",
    "The p-value is the probability for getting a value at least as large as the one observed. So a small p-value (conventionally smaller than 0.05) is used as an indication that the obtained value is not a statistical fluke. One may however not abuse of it / puts too much trust in it (but it is sometimes the only objective way we have to quantify a \"visual\" statement). \n",
    "\n",
    "HERE, we are not testing if there is a strong correlation, but testing that the coefficient we find can happen due to statistical fluctuations. A large positive $r$ means strong positive correlation and large negative means anti-correlation. We care about extreme values in both directions so: \n",
    "$p = 2 \\times P(T \\geq |t|))$ (where $T$ is the t-distribution with $n-2$ dof. \n",
    "\n",
    "HERE, a low p-value does not mean the correlation is weak, it means:  If there were truly no correlation, the chance of observing such an extreme r (positive or negative) is very small.\n",
    "So, a low p-value gives us confidence that the observed correlation (positive or negative) is real, not random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cb19cd-81aa-401d-83d7-4db317d23184",
   "metadata": {},
   "source": [
    "### More about correlation coefficients\n",
    "\n",
    "The Pearson's coefficient can be calculated in python using `numpy.corrcoef()`.  \n",
    "\n",
    "Note that there are two problems with Pearson's coefficient:\n",
    "- It does not incorporate measurement uncertainties on the data.\n",
    "- It is highly susceptible to outliers.\n",
    "\n",
    "Alternative coefficients have been introduced, such as the Spearman's-r coefficient (less sensible to outliers but biased) and the Kendall's-$\\tau$ coeffcient. \n",
    "\n",
    "See **Chapter 3** of of the book <a class=\"anchor\" id=\"book\"></a> *Statistics, data mining and Machine learning in astronomy* by Z. Ivezic et al. in Princeton Series in Modern Astronomy for a more exhaustive discussion of correlation coefficients (and alternatives to Pearson's $r$ correlation coefficient). \n",
    "\n",
    "Why $n-1$ as Bessel's correction for calculating the covariance $\\sigma_{ab}$ (i.e. `np.cov(a, b, ddof=1)`) ? See https://stats.stackexchange.com/questions/142456/why-shouldnt-the-denominator-of-the-covariance-estimator-be-n-2-rather-than-n-1/142472#142472 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2316b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the data\n",
    "data_sample =    # load the data \n",
    "a, b =         # save a, b columns into variables for legibility \n",
    "\n",
    "# Create an axis object to visualise the data points \n",
    "# ADD HERE A row to visualise the data \n",
    "\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('a')\n",
    "ax.set_ylabel('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e572419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the covariance matrix to quantify the visual evidence for a correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f217ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a correlation coefficient and its uncertainty (as a complement to assess the covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p value to see if our value of r could have been as large just from statistical fluctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a5addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the uncertainty on c using the covariance and ignoring it "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_lectures]",
   "language": "python",
   "name": "conda-env-py3_lectures-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
